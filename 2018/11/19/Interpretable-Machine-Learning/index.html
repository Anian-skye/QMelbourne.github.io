<!DOCTYPE html>
<html lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <link rel="stylesheet" href="/style/style.css">
<script>
  var nlviconfig = {
    title: "CuteNian",
    author: "Nian",
    baseUrl: "/",
    theme: {
      scheme: "banderole",
      lightbox: true,
      animate: true,
      search: true,
      friends: false,
      reward: false,
      lazy: false
    }
  }
</script>




    <link rel="stylesheet" href="/script/lib/lightbox/css/lightbox.min.css">




    <link rel="stylesheet" href="/syuanpi/syuanpi.min.css">











<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="browsermode" content="application">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="CuteNian">
<meta name="apple-mobile-web-app-status-bar-style" content="default">
<meta name="msapplication-navbutton-color" content="#666666">
<meta name="format-detection" content="telephone=no">
<meta name="keywords" content="nlvi, Nlvi">





  <meta name="keywords" content="paper, Nlvi">

  <title> Interpretable_Machine_Learning · CuteNian </title>
</head>
<body>
  <div class="progress">
  <div class="progress-inner"></div>
</div>

  
    <div class="tagcloud-mask"></div>  
<div class="tagcloud" id="tagcloud">
  <div class="tagcloud-inner">
    <a href="/tags/DP/" style="font-size: 14px;">DP</a> <a href="/tags/paper/" style="font-size: 14px;">paper</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/体系结构/" style="font-size: 14px;">体系结构</a> <a href="/tags/刷题/" style="font-size: 14px;">刷题</a> <a href="/tags/前端/" style="font-size: 14px;">前端</a> <a href="/tags/拓扑/" style="font-size: 14px;">拓扑</a>
  </div>
</div>
  

  <div class="container" style="display:none;">

    <header class="header" id="header">
  <div class="header-wrapper">
    <div class="logo">
  <div class="logo-inner syuanpi tvIn">
    <span><a href="/">CuteNian</a></span>
    
  </div>
</div>
    <nav class="main-nav">
  
  <ul class="main-nav-list syuanpi tvIn">
  
    <li class="menu-item">
      <a href="javascript:;" id="search">
        <span>Search</span>
        
          <span class="menu-item-label">search</span>
        
      </a>
    </li>
  
  
    
      
    
    <li class="menu-item">
      <a href="/" id="article">
        <span class="base-name">Articles</span>
        
          <span class="menu-item-label">article</span>
        
      </a>
    </li>
  
    
      
    
    <li class="menu-item">
      <a href="/archives" id="archives">
        <span class="base-name">Archives</span>
        
          <span class="menu-item-label">archives</span>
        
      </a>
    </li>
  
    
      
    
    <li class="menu-item">
      <a href="javascript:;" id="tags">
        <span class="base-name">Tags</span>
        
          <span class="menu-item-label">tags</span>
        
      </a>
    </li>
  
    
      
    
    <li class="menu-item">
      <a href="/about" id="about">
        <span class="base-name">About</span>
        
          <span class="menu-item-label">about</span>
        
      </a>
    </li>
  
  </ul>
  
</nav>

    
    
  </div>
</header>
<div class="mobile-header">
  <div class="mobile-header-body">
    <div class="mobile-header-list">
      
        
            <div class="mobile-nav-item">
                <a href="/">
                    <span>Articles</span>
                    
                    
                </a>
            </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/archives">
                    <span>Archives</span>
                    
                    
                </a>
            </div>
        
      
        
          <div class="mobile-nav-item inner-cloud">
            <div class="mobile-nav-tag">
              <a href="javascript:;" id="mobile-tags">
                <span>Tags</span>
                
                
              </a>
            </div>
            <div class="mobile-nav-tagcloud">
              <div class="mobile-tagcloud-inner">
                <a href="/tags/DP/" style="font-size: 14px;">DP</a> <a href="/tags/paper/" style="font-size: 14px;">paper</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/体系结构/" style="font-size: 14px;">体系结构</a> <a href="/tags/刷题/" style="font-size: 14px;">刷题</a> <a href="/tags/前端/" style="font-size: 14px;">前端</a> <a href="/tags/拓扑/" style="font-size: 14px;">拓扑</a>
              </div>
            </div>
          </div>
        
      
        
            <div class="mobile-nav-item">
                <a href="/about">
                    <span>About</span>
                    
                    
                </a>
            </div>
        
      
    </div>
  </div>
  <div class="mobile-header-nav">
    <div class="mobile-header-item" id="mobile-left">
      <div class="header-menu-item">
        <span class="header-menu-line"></span>
      </div>
    </div>
    <h1 class="mobile-header-title">
      <a href="/">CuteNian</a>
    </h1>
    <div class="mobile-header-item"></div>
  </div>
</div>
    <div class="container-inner">
      <main class="main" id="main">
        <div class="main-wrapper">
          
    
  
  <article class="
  post
   is_post 
  ">
    <header class="post-header">
      <div class="post-time syuanpi fadeInUpShort back-1">
        <div class="post-time-wrapper">
          <span>2018-11-19</span>
          
            
              <span class="post-category"><a href="/categories/DP/">DP</a></span>
            
          
          
            
              <aside class="post-tags syuanpi fadeInUpShort back-3">
              
                <a href="/tags/paper/">paper</a>
              
              </aside>
            
          
        </div>
      </div>
      <h1 class="post-title syuanpi fadeInUpShort back-2">
        
          Interpretable_Machine_Learning
        
      </h1>
    </header>
    <div class="post-content syuanpi fadeInUpShort back-3">
      
        <p>机器学习的可解释性</p>
<p>[TOC]</p>
<a id="more"></a>
<h1 id="2-Interpretable"><a href="#2-Interpretable" class="headerlink" title="2. Interpretable"></a>2. Interpretable</h1><h2 id="2-1-Importance"><a href="#2-1-Importance" class="headerlink" title="2.1 Importance"></a>2.1 Importance</h2><p>A correct prediction only partially solves your original problem. The following reasons(Why) drive the demand for interpretability and explanations</p>
<ol>
<li>To facilitate learning and satisfy curiosity as to why certain predictions or behaviors are created by machines, interpretability and explanations are crucial. </li>
<li>The more a machine’s decision affects a person’s life, the more important it will be for the machine to explain its behavior.</li>
<li>Interpretability makes it possible to extract this additional knowledge captured by the model.</li>
<li>Machine learning models take on real-world tasks that require <strong>safety measures</strong> and testing.</li>
<li>Interpretability is a useful debugging tool for <strong>detecting bias</strong> in machine learning models.</li>
<li>The process of integrating machines and algorithms into our daily lives requires interpretability to increase <strong>social acceptance</strong>.</li>
<li>Machine learning models can only be <strong>debugged and audited</strong> when they can be interpreted.</li>
</ol>
<h2 id="2-2-Taxonomy-of-Interpretability-Methods"><a href="#2-2-Taxonomy-of-Interpretability-Methods" class="headerlink" title="2.2 Taxonomy of Interpretability Methods"></a>2.2 Taxonomy of Interpretability Methods</h2><h3 id="Intrinsic-amp-amp-Post-hoc"><a href="#Intrinsic-amp-amp-Post-hoc" class="headerlink" title="Intrinsic &amp;&amp; Post hoc"></a>Intrinsic &amp;&amp; Post hoc</h3><p><strong>Intrinsic</strong> interpretability refers to machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models. </p>
<p><strong>Post hoc</strong> interpretability refers to the application of interpretability methods after model training.</p>
<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><p>The various interpretability methods can be roughly differentiated according to their results.</p>
<ol>
<li>Feature summary statistic</li>
<li>Feature summary visualization</li>
<li><p>Model internals(e.g.:learned weight)</p>
</li>
<li><p>Data point（for images and text)</p>
</li>
<li>Intrinsically interpertable model</li>
</ol>
<h3 id="Model-specific-amp-amp-model-agnostic"><a href="#Model-specific-amp-amp-model-agnostic" class="headerlink" title="Model-specific &amp;&amp; model-agnostic"></a>Model-specific &amp;&amp; model-agnostic</h3><p>Model-specific interpretation tools are limited to specific model classes.(e.g.:intrinsic)</p>
<p>Model-agnostic tools can be used on any machine learning model (e.g.:post hoc)</p>
<h3 id="Local-or-Global"><a href="#Local-or-Global" class="headerlink" title="Local or Global"></a>Local or Global</h3><p>Does the interpretation method explain an individual prediction or the entire model behavior? Or is the scope somewhere in between?</p>
<h2 id="2-3-Scope-of-Interpretability"><a href="#2-3-Scope-of-Interpretability" class="headerlink" title="2.3 Scope of Interpretability"></a>2.3 Scope of Interpretability</h2><h3 id="Algorithm-Transparency"><a href="#Algorithm-Transparency" class="headerlink" title="Algorithm Transparency"></a>Algorithm Transparency</h3><p><em>How does the algorithm create the model?</em></p>
<p>Algorithm transparency only requires knowledge of the algorithm and not of the data or learned models</p>
<h3 id="Global-Holistic-Model-Interpretablity"><a href="#Global-Holistic-Model-Interpretablity" class="headerlink" title="Global,Holistic Model Interpretablity"></a>Global,Holistic Model Interpretablity</h3><p><em>How does the trained model make predictions?</em></p>
<p>To explain the global model output, you need the trained model, knowledge of the algorithm and the data. </p>
<p>Global is very difficulit to achieve in practice</p>
<h3 id="Global-Model-Interpretability-on-a-Modular-Level"><a href="#Global-Model-Interpretability-on-a-Modular-Level" class="headerlink" title="Global Model Interpretability on a Modular Level"></a>Global Model Interpretability on a Modular Level</h3><p><em>How do parts of the model affect predictions?</em></p>
<h3 id="Local-Interpretability-for-a-Single-Prediction"><a href="#Local-Interpretability-for-a-Single-Prediction" class="headerlink" title="Local Interpretability for a Single Prediction"></a>Local Interpretability for a Single Prediction</h3><p><em>Why did the model make a certain prediction for an instance?</em></p>
<p>You can zoom in on a single instance and examine what kind of prediction the model makes for this input, and explain why it made this decision.</p>
<h3 id="Local-Interpretability-for-a-Group-of-Predictions"><a href="#Local-Interpretability-for-a-Group-of-Predictions" class="headerlink" title="Local Interpretability for a Group of Predictions"></a>Local Interpretability for a Group of Predictions</h3><p><em>Why did the model make specific predictions for a group of instances?</em></p>
<h2 id="2-4-Evaluating-Interpretability"><a href="#2-4-Evaluating-Interpretability" class="headerlink" title="2.4 Evaluating Interpretability"></a>2.4 Evaluating Interpretability</h2><p>Doshi-Velez and Kim (2017) propose three main levels for the evaluation of interpretability:</p>
<h3 id="Application-level-evaluation-real-task"><a href="#Application-level-evaluation-real-task" class="headerlink" title="Application level evaluation(real task)"></a>Application level evaluation(real task)</h3><p>Put the explanation into the product and have it tested by the end user.</p>
<h3 id="Human-level-evaluation-simple-task"><a href="#Human-level-evaluation-simple-task" class="headerlink" title="Human level evaluation(simple task)"></a>Human level evaluation(simple task)</h3><p> A simplified application level evaluation.</p>
<p> The difference is that these experiments are not carried out with the domain experts, but with laypersons.</p>
<h3 id="Function-level-evaluation-proxy-task"><a href="#Function-level-evaluation-proxy-task" class="headerlink" title="Function level evaluation (proxy task)"></a>Function level evaluation (proxy task)</h3><p>Does not require humans. </p>
<h2 id="2-5-Properties-of-Explanations"><a href="#2-5-Properties-of-Explanations" class="headerlink" title="2.5 Properties of Explanations"></a>2.5 Properties of Explanations</h2><p>These properties can be used to judge how good an explanation (method) is. It’s not clear for all these properties how to measure them correctly, so one of the challenges is to formalize how they could be calculated.</p>
<p><strong>Properties of Explanation Methods</strong></p>
<ol>
<li><strong>Express power </strong>is the “language” or structure of the explanations the method is able to generate. </li>
<li><strong>Translucency</strong> describes how much the explanation method relies on looking into the machine learning model, like its parameters. </li>
<li><strong>Portability</strong> describes the range of machine learning models with which the explanation method can be used. </li>
<li><strong>Algorithmic Complexity</strong> describes the computational complexity of the method that generates the explanation. </li>
</ol>
<p><strong>Properties of Individual Explanations</strong></p>
<ol>
<li><strong>Accuracy</strong>: How well does an explanation predict unseen data?</li>
<li><strong>Fidelity</strong>: How well does the explanation approximate the prediction of the black box model?</li>
<li><strong>Consistency</strong>: How much does an explanation differ between models that have been trained on the same task and that produce similar predictions?</li>
<li><strong>Stability</strong>: How similar are the explanations for similar instances? </li>
<li><strong>Comprehensibility</strong>: How well do humans understand the explanations?</li>
<li><strong>Certainty</strong>: Does the explanation reflect the certainty of the machine learning model?</li>
<li><strong>Degree of Importance</strong>: How well does the explanation reflect the importance of features or parts of the explanation?</li>
<li><p><strong>Novelty</strong>: Does the explanation reflect whether a data instance to be explained comes from a “new” region far removed from the distribution of training data?</p>
</li>
<li><p><strong>Representativeness</strong>: How many instances does an explanation cover?</p>
</li>
</ol>
<h2 id="2-6-Human-friendly-Explanations"><a href="#2-6-Human-friendly-Explanations" class="headerlink" title="2.6 Human-friendly Explanations"></a>2.6 Human-friendly Explanations</h2><h3 id="What-is-a-good-explanation"><a href="#What-is-a-good-explanation" class="headerlink" title="What is a good explanation"></a>What is a good explanation</h3><ol>
<li><strong>Explanations are contrastive</strong>: Humans usually don’t ask why a certain prediction was made, but rather why this prediction was made instead of another prediction. </li>
<li><strong>Explanations are selected</strong> :<strong>What it means for interpretable machine learning</strong>: Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex. </li>
<li><strong>Explanations are social</strong>: Be mindful of the social setting of your machine learning application and of the target audience.</li>
<li><strong>Explanations focus on the abnormal</strong>. If one of the input features for a prediction was abnormal in any sense (like a rare category of a categorical feature) and the feature influenced the prediction, it should be included in an explanation, even if other ‘normal’ features have the same influence on the prediction as the abnormal one. </li>
<li><strong>Explanations are truthful</strong>. The explanation should predict the event as truthfully as possible, which is sometimes called <strong>fidelity</strong> in the context of machine learning. </li>
<li><strong>Good explanations are coherent with prior beliefs of the explainee</strong>.</li>
<li><strong>Good explanations are general and probable</strong>.</li>
</ol>
<h1 id="3-Datasets"><a href="#3-Datasets" class="headerlink" title="3. Datasets"></a>3. Datasets</h1><ol>
<li>Bike sharing counts(Regression)</li>
<li>YouTube Spam Comments(Text Classification)</li>
<li>Risk Factors For Cervical Cancer(Classfication)</li>
</ol>
<h1 id="4-Interpretable-Models"><a href="#4-Interpretable-Models" class="headerlink" title="4. Interpretable Models"></a>4. Interpretable Models</h1><p><img src="/var/folders/6p/_f33fscs7j316s490d30bm6c0000gn/T/abnerworks.Typora/image-20181206161452656.png" alt="image-20181206161452656"></p>
<h2 id="4-1-Linear-Regression-Model"><a href="#4-1-Linear-Regression-Model" class="headerlink" title="4.1 Linear Regression Model"></a>4.1 Linear Regression Model</h2><h3 id="Interpretation"><a href="#Interpretation" class="headerlink" title="Interpretation"></a>Interpretation</h3><p> $R^{2}$ measurement.  $R^{2}$ tells you how much of the total variance of your target outcome is explained by the model. The higher  $R^{2}$ the better your model explains the data.</p>
<p>$R^{2} = 1 - SSE/SST$</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fxx3rsaex1j306801m746.jpg" alt="image-20181206162539535"></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fxx3rxt86yj306401z746.jpg" alt="image-20181206162549601"></p>
<p>There is a catch, because R2R2 increases with the number of features in the model, even if they carry no information about the target value at all. So it is better to use the adjusted R-squared, which accounts for the number of features used in the model. </p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fxx3t0q9drj30eu01xt8m.jpg" alt="image-20181206162652114"></p>
<p>while p is the number of feature and n the number of instances.</p>
<p>The importance of a feature in a linear regression model can be measured by the absolute value of its t-statistic. The t-statistic is the estimated weight scaled with it’s standard error.</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fxx3un9dn6j3042025dfp.jpg" alt="image-20181206162824717"></p>
<h1 id="5-Model-Agnostic-Methods"><a href="#5-Model-Agnostic-Methods" class="headerlink" title="5. Model-Agnostic Methods"></a>5. Model-Agnostic Methods</h1><h2 id="5-1-Partial-Dependence-Plot-PDP"><a href="#5-1-Partial-Dependence-Plot-PDP" class="headerlink" title="5.1 Partial Dependence Plot(PDP)"></a>5.1 Partial Dependence Plot(PDP)</h2><p>The partial dependence plot (PDP or PD plot) shows the marginal effect of a feature on the predicted outcome of a previously fit model.</p>
<p>For classification, where the machine model outputs probabilities, the partial dependence function displays the probability for a certain class given different values for features xSxS. A straightforward way to handle multi-class problems is to plot one line or one plot per class.</p>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>In practice, the set of features xSxS usually only contains one feature or a maximum of two, because one feature produces 2D plots and two features produce 3D plots.</p>
<p>The influence of the weather features on the predicted bike counts:</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fxx5nm3m50j30m40fjt9i.jpg" alt="image-20181206173039784"></p>
<h3 id="Advantages-amp-Disadvantages"><a href="#Advantages-amp-Disadvantages" class="headerlink" title="Advantages &amp; Disadvantages"></a>Advantages &amp; Disadvantages</h3><h4 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h4><ol>
<li>The computation of partial dependence plots is <strong>intuitive</strong>: </li>
<li>If the feature for which you computed the PDP is uncorrelated with the other model features, then the PDPs are perfectly representing how the feature influences the target on average.</li>
<li>Partial dependence plots are <strong>simple to implement</strong>.</li>
<li><strong>Causal interpretation</strong></li>
</ol>
<h4 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><ol>
<li><p>The <strong>maximum number of features</strong> you can look at jointly is realistically two or three</p>
</li>
<li><p>Some PD plots don’t include the <strong>feature distribution</strong>. </p>
</li>
<li>The <strong>assumption of independence</strong> poses the biggest issue of PD plots.</li>
<li><strong>Heterogeneous effects might be hidden</strong></li>
</ol>
<h2 id="5-2-Individual-Conditional-Expectation"><a href="#5-2-Individual-Conditional-Expectation" class="headerlink" title="5.2 Individual Conditional Expectation"></a>5.2 Individual Conditional Expectation</h2><p>For a chosen feature, Individual Conditional Expectation (ICE) plots draw one line per instance, representing how the instance’s prediction changes when the feature changes.</p>
<p>ICE is the equivalent to a PDP for local expectations.</p>
<p>An ICE plot visualizes the dependence of the predicted response on a feature for EACH instance separately, resulting in multiple lines, one for each instance, compared to one line in partial dependence plots. A PDP is the average of the lines of an ICE plot. </p>
<h3 id="Advantages-amp-Disadvantages-1"><a href="#Advantages-amp-Disadvantages-1" class="headerlink" title="Advantages &amp; Disadvantages"></a>Advantages &amp; Disadvantages</h3><h4 id="Advantages-1"><a href="#Advantages-1" class="headerlink" title="Advantages"></a>Advantages</h4><ol>
<li><strong>more intuitive to understand</strong></li>
<li>In contrast to partial dependence plots they can <strong>uncover heterogeneous relationships</strong>.</li>
</ol>
<h4 id="Disadvantages-1"><a href="#Disadvantages-1" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><ol>
<li><strong>can only display one feature</strong> meaningfully</li>
<li>the plot <strong>can become overcrowded</strong></li>
<li>When the feature of interest is correlated with the other features, then <strong>not all points in the lines might be valid data points</strong> according to the joint feature distribution.</li>
<li>In ICE plots it might not be easy to <strong>see the average</strong>. </li>
</ol>
<h2 id="5-3-Accumulated-Local-Effects-ALE-Plot"><a href="#5-3-Accumulated-Local-Effects-ALE-Plot" class="headerlink" title="5.3 Accumulated Local Effects (ALE) Plot"></a>5.3 Accumulated Local Effects (ALE) Plot</h2><p>ALE plots are a faster and unbiased alternative to <a href="https://christophm.github.io/interpretable-ml-book/pdp.html#pdp" target="_blank" rel="noopener">partial dependence plots</a></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fxyeluunymj30op0fb765.jpg" alt="image-20181207192559970"></p>
<h3 id="Advantages-amp-Disadvantages-2"><a href="#Advantages-amp-Disadvantages-2" class="headerlink" title="Advantages &amp; Disadvantages"></a>Advantages &amp; Disadvantages</h3><h4 id="Advantages-2"><a href="#Advantages-2" class="headerlink" title="Advantages"></a>Advantages</h4><ol>
<li><strong>ALE plots are unbiased</strong>, which means they still work when features are correlated.</li>
<li><strong>ALE plots are faster to compute</strong> than PDPs </li>
<li>The <strong>interpretation of ALE plots is clear</strong></li>
</ol>
<p><strong>Use accumulated local effect plots, dump partial dependence plots.</strong></p>
<h2 id="5-4-Feature-Interaction"><a href="#5-4-Feature-Interaction" class="headerlink" title="5.4  Feature Interaction"></a>5.4  Feature Interaction</h2><p>One way to estimate the interaction strength is to measure how much of the variation of the predicted outcome depends on the interaction of the features. </p>
<p>H-statistics</p>
<h2 id="5-5-Feature-Importance"><a href="#5-5-Feature-Importance" class="headerlink" title="5.5 Feature Importance"></a>5.5 Feature Importance</h2><p>A feature’s importance is the increase in the model’s prediction error after we permuted the feature’s values。</p>
<p>A feature is “important” if permuting its values increases the model error, because the model relied on the feature for the prediction. A feature is “unimportant” if permuting its values keeps the model error unchanged, because the model ignored the feature for the prediction.</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fxz4yoyu5vj30m40fl75x.jpg" alt="image-20181208103758812"></p>
<h3 id="Advantages-amp-Disadvantages-3"><a href="#Advantages-amp-Disadvantages-3" class="headerlink" title="Advantages &amp; Disadvantages"></a>Advantages &amp; Disadvantages</h3><h4 id="Advantages-3"><a href="#Advantages-3" class="headerlink" title="Advantages"></a>Advantages</h4><ol>
<li>Nice interpretation</li>
<li>Feature importance provides a highly compressed, global insight into the model’s behavior.</li>
<li>The importance measure automatically takes into account all interactions with other features. </li>
<li>Permutation feature importance doesn’t require retraining the model like.</li>
</ol>
<h4 id="Disadvantages-2"><a href="#Disadvantages-2" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><ol>
<li>It’s very unclear whether you should use training or test data for computing the feature importance.</li>
<li>The feature importance measure is tied to the error of the model. </li>
<li>You need access to the actual outcome target. </li>
</ol>
<h2 id="5-6-Global-Surrogate-Models"><a href="#5-6-Global-Surrogate-Models" class="headerlink" title="5.6  Global Surrogate Models"></a>5.6  Global Surrogate Models</h2><p>A global surrogate model is an interpretable model that is trained to approximate the predictions of a black box model.</p>
<h3 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h3><p> there is actually not much theory needed to understand surrogate models. We want to approximate our black box prediction function $\mathop f (x)\ $as closely as possible with the surrogate model prediction function $g(x)$, under the constraint that gg is interpretable. Any interpretable model - for example from the <a href="https://christophm.github.io/interpretable-ml-book/simple.html#simple" target="_blank" rel="noopener">interpretable models chapter</a> - can be used for the function $g(x)$  (linear model,decision tree)</p>
<p>Fitting a surrogate model only needs the relation of input and predicted output.</p>
<p>Perform the following steps to get a surrogate model:</p>
<ol>
<li>Choose a dataset $X$. This could be the same dataset that was used for training the black box model or a new dataset from the same distribution. You could even choose a subset of the data or a grid of points, depending on your application.</li>
<li>For the chosen dataset $X$, get the predictions $Y$ of the black box model.</li>
<li>Choose an interpretable model (linear model, decision tree, …).</li>
<li>Train the interpretable model on the dataset $X$ and its predictions $Y$.</li>
<li>Congratulations! You now have a surrogate model.</li>
<li>Measure how well the surrogate model replicates the prediction of the black box model.</li>
<li>Interpret / visualize the surrogate model.</li>
</ol>
<p>A way to measure how well the surrogate replicates the black box model is the R squared measure.</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fxz6okksmqj309j01w3yh.jpg" alt="image-20181208113730687"></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fxz6trs93mj30lz0663zu.jpg" alt="image-20181208114229675"></p>
<p>If the black box model sucks,the interpretation of the surrogate model is  still valid,because it makes statements about the model and not about the real world.</p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>data: daliy numbers of rented bikes(weather,calendrical information)</p>
<p>f(x):SVM</p>
<p>g(x):decision tree</p>
<p>The surrogate model has an R squared (variance explained) of 0.77 which means it approximates the underlying black box behavior quite well, but not perfectly. </p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fxz9kxlc9cj30lc0eiwfp.jpg" alt="image-20181208131748280"></p>
<p>data: cervical cancer</p>
<p>f(x): random forest</p>
<p>g(x): decision tree</p>
<p>The surrogate model has an R squared (variance explained) of 0.2 which means it doesn’t approximate the random forest well and we should not over-interpret the tree, when drawing conclusions about the complex model.</p>
<h3 id="Advantages-amp-Disadvantages-4"><a href="#Advantages-amp-Disadvantages-4" class="headerlink" title="Advantages &amp; Disadvantages"></a>Advantages &amp; Disadvantages</h3><h4 id="Advantages-4"><a href="#Advantages-4" class="headerlink" title="Advantages"></a>Advantages</h4><ol>
<li>The surrogate model method is <strong>flexible</strong></li>
<li>I’d argue that the approach is very <strong>intuitive</strong> and straightforward.</li>
<li>With the R squard measure, we can easily <strong>measure</strong> how good our surrogate models are in terms of approximation of the black box predictions.</li>
</ol>
<h4 id="Disadvantages-3"><a href="#Disadvantages-3" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><ol>
<li>Be careful to draw <strong>conclusions about the model, not the data</strong>, since the surrogate model never sees the real outcome.</li>
<li>It’s not clear what the best <strong>cut-off for R squared</strong> is in order to be confident that the surrogate model is close enough to the black box model. 80% of variance explained? 50%? 99%?</li>
<li>It could happen that the interpretable model is very <strong>close for a subset of the dataset, but wildly diverges for another subset</strong>. In this case the interpretation for the simple model would not be equally good for all data points.</li>
<li>The interpretable model you choose as a surrogate <strong>comes with all its advantages and disadvantages</strong>.</li>
</ol>
<h2 id="5-7-Local-Surrogate-Models"><a href="#5-7-Local-Surrogate-Models" class="headerlink" title="5.7 Local Surrogate Models"></a>5.7 Local Surrogate Models</h2><p>Local surrogate models are interpretable models used to explain individual predictions of black box machine learning models.</p>
<p> Instead of trying to fit a global surrogate model, LIME focuses on fitting local surrogate models to explain why single predictions were made.</p>
<p>data:consisting of perturbed samples and the associated black box model’s predictions.</p>
<p>On this dataset LIME then trains an interpretable model weighted by the proximity of the sampled instances to the instance of interest.</p>
<p>Mathematically, local surrogate models with interpretability constraint can be expressed as follows:</p>
<p>​            $\text{explanation}(x)=\arg\min_{g\in{}G}L(f,g,\pi_x)+\Omega(g)$</p>
<p>The recipe for fitting local surrogate models:</p>
<ol>
<li>Choose your instance of interest for which you want to have an explanation of its black box prediction.</li>
<li>Perturb your dataset and get the black box predictions for these new points.</li>
<li>Weight the new samples by their proximity to the instance of interest.</li>
<li>Fit a weighted, interpretable model on the dataset with the variations.</li>
<li>Explain prediction by interpreting the local model.</li>
</ol>
<p>How to get the variations of the data:</p>
<p>This differs depending on the type of data, which can be either text, an image or tabular data.</p>
<h3 id="LIME-for-tabular-Data"><a href="#LIME-for-tabular-Data" class="headerlink" title="LIME for tabular Data"></a>LIME for tabular Data</h3><p>Tabular data means any data that comes in tables, where each row represents an instance and each column a feature.</p>
<p> LIME sampling is not done around the instance of interest, but from the training data’s mass centre, which is problematic. But it increases the likelihood that the outcome for some of the sampled points predictions differ from the data point of interest and that LIME can learn at least some explanation.</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fxzc8vciogj30nd0l6gs9.jpg" alt="image-20181208145001064"></p>
<h3 id="LIME-for-Text"><a href="#LIME-for-Text" class="headerlink" title="LIME for Text"></a>LIME for Text</h3><p>Starting from the original text, new texts are created by randomly removing words from it. The dataset is represented with binary features for each word. A feature is 1 if the respective word is included and 0 if it was removed.</p>
<h3 id="LIME-for-image"><a href="#LIME-for-image" class="headerlink" title="LIME for image"></a>LIME for image</h3><p>variations of the samples (i.e. images) are created by performing superpixel segmentation and switching superpixels off. </p>
<p>(Superpixels are connected pixels with similar colors and can be turned off by replacing each pixel by a user provided color)</p>
<h3 id="Advantages-amp-Disadvantages-5"><a href="#Advantages-amp-Disadvantages-5" class="headerlink" title="Advantages &amp; Disadvantages"></a>Advantages &amp; Disadvantages</h3><h4 id="Advantages-5"><a href="#Advantages-5" class="headerlink" title="Advantages"></a>Advantages</h4><ol>
<li>Even if you exchange the underlying machine learning model, you can still use the same local, interpretable model for explanation. </li>
<li>Local surrogate models benefit from the literature and experience of training and interpreting interpretable models.</li>
<li>When using LASSO or short trees, the resulting explanations are short (=selective) and possibly contrastive.</li>
<li>LIME is one of the few methods that works for tabular data, texts and images.</li>
</ol>
<h4 id="Disadvantages-4"><a href="#Disadvantages-4" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><ol>
<li>The correct definition of the neighbourhood is a very big, unsolved problem when using LIME with tabular data.</li>
<li>Sampling could be improved in the current implementation of LIME.</li>
<li>The complexity of the explanation model has to be defined in advance.</li>
<li>The instability of the explanations.</li>
</ol>
<h2 id="5-8-Shapley-Value-Explanations"><a href="#5-8-Shapley-Value-Explanations" class="headerlink" title="5.8 Shapley Value Explanations"></a>5.8 Shapley Value Explanations</h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote>
<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/" target="_blank" rel="noopener">https://christophm.github.io/interpretable-ml-book/</a></li>
</ul>
</blockquote>

      
    
    </div>
    
      

      
  <hr class="copy-line">
  <div class="post-copyright">
    <div class="copy-author">
      <span>Author :</span>
      <span>Nian</span>
    </div>
    <div class="copy-url">
      <span>Url :</span>
      <a href="http://cuteanian.top/2018/11/19/Interpretable-Machine-Learning/">http://cuteanian.top/2018/11/19/Interpretable-Machine-Learning/</a>
    </div>
    <div class="copy-origin">
      <span>Origin :</span>
      <a href="http://cuteanian.top">http://cuteanian.top</a>
    </div>
    <div class="copy-license">
      
      著作权归作者所有，转载请联系作者获得授权。
    </div>
  </div>

    
  </article>
  
    
  <nav class="article-page">
    
      <a href="/2018/11/20/leetcode46-Permutations/" id="art-left" class="art-left">
        <span class="next-title">
          <i class="iconfont icon-left"></i>leetcode46_Permutations
        </span>
      </a>
    
    
      <a href="/2018/11/19/leetcode167TwoSum/" id="art-right" class="art-right">
        <span class="prev-title"> 
          leetcode167TwoSum<i class="iconfont icon-right"></i>  
        </span>
      </a>
    
  </nav>

    
  <i id="com-switch" class="iconfont icon-more jumping-in long infinite" style="font-size:24px;display:block;text-align:center;transform:rotate(180deg);"></i>
  <div class="post-comments" id="post-comments" style="display: block;margin: auto 16px;">
    
    
    

    

  </div>



  
  
    
  
  <aside class="post-toc">
    <span class="title" id="toc-switch"><span>Index</span></span>
    <div class="toc-inner syuanpi back-1" style="display:none;">
      <li class="title-link"><a href="javascript:;" class="toTop">Interpretable_Machine_Learning</a></li>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Interpretable"><span class="toc-text">2. Interpretable</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Importance"><span class="toc-text">2.1 Importance</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Taxonomy-of-Interpretability-Methods"><span class="toc-text">2.2 Taxonomy of Interpretability Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Intrinsic-amp-amp-Post-hoc"><span class="toc-text">Intrinsic &amp;&amp; Post hoc</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Result"><span class="toc-text">Result</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-specific-amp-amp-model-agnostic"><span class="toc-text">Model-specific &amp;&amp; model-agnostic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Local-or-Global"><span class="toc-text">Local or Global</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Scope-of-Interpretability"><span class="toc-text">2.3 Scope of Interpretability</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Algorithm-Transparency"><span class="toc-text">Algorithm Transparency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Global-Holistic-Model-Interpretablity"><span class="toc-text">Global,Holistic Model Interpretablity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Global-Model-Interpretability-on-a-Modular-Level"><span class="toc-text">Global Model Interpretability on a Modular Level</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Local-Interpretability-for-a-Single-Prediction"><span class="toc-text">Local Interpretability for a Single Prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Local-Interpretability-for-a-Group-of-Predictions"><span class="toc-text">Local Interpretability for a Group of Predictions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-Evaluating-Interpretability"><span class="toc-text">2.4 Evaluating Interpretability</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Application-level-evaluation-real-task"><span class="toc-text">Application level evaluation(real task)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Human-level-evaluation-simple-task"><span class="toc-text">Human level evaluation(simple task)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Function-level-evaluation-proxy-task"><span class="toc-text">Function level evaluation (proxy task)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-Properties-of-Explanations"><span class="toc-text">2.5 Properties of Explanations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-Human-friendly-Explanations"><span class="toc-text">2.6 Human-friendly Explanations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-a-good-explanation"><span class="toc-text">What is a good explanation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Datasets"><span class="toc-text">3. Datasets</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Interpretable-Models"><span class="toc-text">4. Interpretable Models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Linear-Regression-Model"><span class="toc-text">4.1 Linear Regression Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interpretation"><span class="toc-text">Interpretation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Model-Agnostic-Methods"><span class="toc-text">5. Model-Agnostic Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-Partial-Dependence-Plot-PDP"><span class="toc-text">5.1 Partial Dependence Plot(PDP)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Examples"><span class="toc-text">Examples</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-amp-Disadvantages"><span class="toc-text">Advantages &amp; Disadvantages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Advantages"><span class="toc-text">Advantages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Disadvantages"><span class="toc-text">Disadvantages</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Individual-Conditional-Expectation"><span class="toc-text">5.2 Individual Conditional Expectation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-amp-Disadvantages-1"><span class="toc-text">Advantages &amp; Disadvantages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Advantages-1"><span class="toc-text">Advantages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Disadvantages-1"><span class="toc-text">Disadvantages</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Accumulated-Local-Effects-ALE-Plot"><span class="toc-text">5.3 Accumulated Local Effects (ALE) Plot</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-amp-Disadvantages-2"><span class="toc-text">Advantages &amp; Disadvantages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Advantages-2"><span class="toc-text">Advantages</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-Feature-Interaction"><span class="toc-text">5.4  Feature Interaction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-Feature-Importance"><span class="toc-text">5.5 Feature Importance</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-amp-Disadvantages-3"><span class="toc-text">Advantages &amp; Disadvantages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Advantages-3"><span class="toc-text">Advantages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Disadvantages-2"><span class="toc-text">Disadvantages</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-6-Global-Surrogate-Models"><span class="toc-text">5.6  Global Surrogate Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Theory"><span class="toc-text">Theory</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Example"><span class="toc-text">Example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-amp-Disadvantages-4"><span class="toc-text">Advantages &amp; Disadvantages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Advantages-4"><span class="toc-text">Advantages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Disadvantages-3"><span class="toc-text">Disadvantages</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-7-Local-Surrogate-Models"><span class="toc-text">5.7 Local Surrogate Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LIME-for-tabular-Data"><span class="toc-text">LIME for tabular Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LIME-for-Text"><span class="toc-text">LIME for Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LIME-for-image"><span class="toc-text">LIME for image</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advantages-amp-Disadvantages-5"><span class="toc-text">Advantages &amp; Disadvantages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Advantages-5"><span class="toc-text">Advantages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Disadvantages-4"><span class="toc-text">Disadvantages</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-8-Shapley-Value-Explanations"><span class="toc-text">5.8 Shapley Value Explanations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-text">参考资料</span></a></li></ol></li></ol>
    </div>
  </aside>



  


        </div>
      </main>

      <footer class="footer syuanpi fadeIn" id="footer">
  <hr>
  <div class="footer-wrapper">
    <div class="left">
      <div class="contact-icon">
    
    
    
    
    
    
    
    
        
        
        
        
        
            <a href="https://www.facebook.com/1" class="iconfont icon-facebook" title="facebook"></a>
        
        
        
    
        
        
        
            <a href="https://twitter.com/2" class="iconfont icon-twitter" title="twitter"></a>
        
        
        
        
        
    
        
        
        
        
        
        
        
            <a href="https://www.instagram.com/3" class="iconfont icon-ins" title="instagram"></a>
        
    
        
        
            <a href="https://weibo.com/4" class="iconfont icon-weibo" title="weibo"></a>
        
        
        
        
        
        
    
        
        
        
        
            <a href="https://www.zhihu.com/people/5" class="iconfont icon-zhihu" title="zhihu"></a>
        
        
        
        
    
        
            <a href="https://github.com/6" class="iconfont icon-github" title="github"></a>
        
        
        
        
        
        
        
    
        
        
        
        
        
        
            <a href="https://www.linkedin.com/in/7" class="iconfont icon-linkedin" title="linkedin"></a>
        
        
    
</div>
    </div>
    <div class="right">
      <div class="copyright">
    <div class="info">
        <span>&copy;</span>
        <span>2016 ~ 2018</span>
        <span>❤</span>
        <span>Nian</span>
    </div>
    <div class="theme">
        <span>
            Powered by
            <a href="http://hexo.io/" target="_blank">Hexo </a>
        </span>
        <span>
            Theme
            <a href="https://github.com/ColMugX/hexo-theme-Nlvi"> Nlvi </a>
        </span>
    </div>
    
</div>

    </div>
  </div>
</footer>
    </div>
  </div>
  <script src="/script/lib/jquery/jquery-3.2.1.min.js"></script>


  <script src="/script/lib/lightbox/js/lightbox.min.js"></script>








<script src="/script/src/nlvi.js"></script>

  <script src="/script/scheme/banderole.js"></script>

<script src="/script/bootstarp.js"></script>


<div class="backtop syuanpi melt toTop" id="backtop">
    <i class="iconfont icon-up"></i>
    <span style="text-align:center;font-family:Georgia;"><span style="font-family:Georgia;" id="scrollpercent">1</span>%</span>
</div>


  <div class="search" id="search">
    <div class="mask" id="mask"></div>
    <div class="search-wrapper syuanpi">
      <h2 id="search-header" class="syuanpi">搜索一下？</h2>
      <div class="input">
        <input type="text" id="local-search-input" results="0" name="">
      </div>
      <div id="local-search-result"></div>
    </div>
  </div>


</body>
</html>
